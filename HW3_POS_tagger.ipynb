{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fxFORMoSO-C6bcMr7GsGcWQZc88_j1FP",
      "authorship_tag": "ABX9TyP7eFutO4hSQ7PnOKYnMSdm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT_87J1f_oQU",
        "outputId": "dceb6e59-c36b-41fc-f908-51159abaf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "QwlvzEOW76QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .txt 파일을 (문장, POS) 리스트로 변환\n",
        "def txt2list(filename):\n",
        "  f = open(filename, 'r')\n",
        "  lines = f.readlines()\n",
        "  f.close()\n",
        "  raw = ''.join(lines)\n",
        "  sents_raw = raw.split('\\n\\n')\n",
        "  words_raw=[]\n",
        "  for sent in sents_raw:\n",
        "    words_raw.append(sent.split('\\n'))\n",
        "  words_raw=words_raw[:-1]\n",
        "  tokenized_list = []\n",
        "  for words in words_raw:\n",
        "    w_list, pos_list = [],[]\n",
        "    for word in words:\n",
        "      w, pos = tuple(word.split('\\t'))\n",
        "      w_list.append(w.lower()) #소문자화\n",
        "      pos_list.append(pos)\n",
        "    tokenized_list.append((w_list, pos_list))\n",
        "  return tokenized_list"
      ],
      "metadata": {
        "id": "Vp5mPSOk5f3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/train.txt')\n",
        "valid_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/valid.txt')\n",
        "test_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/test.txt')"
      ],
      "metadata": {
        "id": "60FSztT76HO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxk2VvFV6KWY",
        "outputId": "b7db80ea-5b91-49a8-f806-194dfada7231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['swansea', '1', 'lincoln', '2'], ['NN', 'CD', 'NNP', 'CD'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list2textpos(list_):\n",
        "  text, pos = [], []\n",
        "  for t in list_:\n",
        "    text.append(t[0])\n",
        "    pos.append(t[1])\n",
        "  return text, pos"
      ],
      "metadata": {
        "id": "z5vN4hA_JWwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_text, train_pos), (test_text, test_pos), (valid_text, valid_pos) = tuple(map(list2textpos, (train_list, test_list, valid_list)))"
      ],
      "metadata": {
        "id": "BJUqMdyYKSkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_bag=[]\n",
        "for p in train_pos: pos_bag+=p\n",
        "bag = ['<PAD>']+list(set(pos_bag))"
      ],
      "metadata": {
        "id": "UJg0r_TjJKA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS 전체 목록 확인\n"
      ],
      "metadata": {
        "id": "NTD6NnrG-1xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BOheDaW8K1",
        "outputId": "0492ebb4-f460-458a-ea11-7b3c3ad6013a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<PAD>',\n",
              " 'VBZ',\n",
              " 'WP$',\n",
              " ':',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'POS',\n",
              " 'SYM',\n",
              " 'RBR',\n",
              " 'EX',\n",
              " 'CD',\n",
              " 'LS',\n",
              " 'NN|SYM',\n",
              " 'DT',\n",
              " 'VBN',\n",
              " 'PDT',\n",
              " 'WP',\n",
              " 'JJR',\n",
              " 'RP',\n",
              " 'RBS',\n",
              " 'WDT',\n",
              " 'NNP',\n",
              " 'UH',\n",
              " 'MD',\n",
              " 'JJS',\n",
              " 'WRB',\n",
              " ',',\n",
              " 'CC',\n",
              " '$',\n",
              " 'NNPS',\n",
              " 'TO',\n",
              " \"''\",\n",
              " 'IN',\n",
              " 'VBP',\n",
              " 'VBG',\n",
              " ')',\n",
              " 'VB',\n",
              " 'PRP$',\n",
              " 'FW',\n",
              " 'NN',\n",
              " 'JJ',\n",
              " '\"',\n",
              " '(',\n",
              " 'RB',\n",
              " 'VBD']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos2label(pos):\n",
        "  pos_=[]\n",
        "  for pos_list in pos:\n",
        "    pos_.append(list(map(bag.index, pos_list)))\n",
        "  return pos_"
      ],
      "metadata": {
        "id": "wMjfp2fZLLqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_pos, test_pos, valid_pos) = tuple(map(pos2label, (train_pos, test_pos, valid_pos)))"
      ],
      "metadata": {
        "id": "WYlx5GhcLwFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCAB 구성"
      ],
      "metadata": {
        "id": "6LhtoDq6dWLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "words_list=[]\n",
        "for t in train_text:\n",
        "  words_list+=t\n",
        "words_counts = Counter(words_list)"
      ],
      "metadata": {
        "id": "YFTPUfeFMh-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(words_counts, key=words_counts.get, reverse=True)"
      ],
      "metadata": {
        "id": "0Po3lSWhRo-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2i={}\n",
        "for index, word in enumerate(['<PAD>', '<UNK>']+vocab):\n",
        "  w2i[word] = index"
      ],
      "metadata": {
        "id": "igVP9cBnR2Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def t2seq(texts, w2i):\n",
        "  encoded_texts=[]\n",
        "  for text in texts:\n",
        "    index_seq = []\n",
        "    for word in text:\n",
        "      try: index_seq.append(w2i[word])\n",
        "      except KeyError: index_seq.append(w2i['<UNK>'])\n",
        "    encoded_texts.append(index_seq)\n",
        "  return encoded_texts"
      ],
      "metadata": {
        "id": "jR-Knnf2TC6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_X_train = t2seq(train_text, w2i)\n",
        "encoded_y_train = train_pos\n",
        "encoded_X_test = t2seq(test_text, w2i)\n",
        "encoded_y_test = test_pos\n",
        "encoded_X_valid = t2seq(valid_text, w2i)\n",
        "encoded_y_valid = valid_pos"
      ],
      "metadata": {
        "id": "-HKJVjraUv5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(map(len, encoded_X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfNQU-8bOat",
        "outputId": "2db335d8-4827-45cf-be83-8adac52115e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "최대 길이가 113이므로 120개 토큰 이하를 패딩한다."
      ],
      "metadata": {
        "id": "Mzv5jjRaNeBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=120\n",
        "\n",
        "def pad_seq(encoded, max_len):\n",
        "  features = np.zeros((len(encoded), max_len), dtype=int)\n",
        "  for i, sent in enumerate(encoded):\n",
        "    features[i, :len(sent)] = np.array(sent)[:max_len]\n",
        "  return features"
      ],
      "metadata": {
        "id": "Tpx7FawOaTQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_X_train = pad_seq(encoded_X_train, max_len=max_len)\n",
        "padded_y_train = pad_seq(encoded_y_train, max_len=max_len)\n",
        "padded_X_test = pad_seq(encoded_X_test, max_len=max_len)\n",
        "padded_y_test = pad_seq(encoded_y_test, max_len=max_len)\n",
        "padded_X_valid = pad_seq(encoded_X_valid, max_len=max_len)\n",
        "padded_y_valid = pad_seq(encoded_y_valid, max_len=max_len)"
      ],
      "metadata": {
        "id": "YELsmnMUbtwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmNGaTfxtNzL",
        "outputId": "a5c5558a-a077-49cb-8f94-dc8a14ac83c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[989, 10951, 205, 629, 7, 3939, 216, 5774, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkVzRVZ6s-lg",
        "outputId": "a082b3d4-2355-4a64-90f8-b79a4e4664be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22, 1, 41, 40, 31, 37, 41, 40, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bidirectional LSTM**"
      ],
      "metadata": {
        "id": "pEraFXYj--Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=2):\n",
        "        super(NERTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim) #bidirectional이므로 hidden dimension이 두 배!! (concatenation의 결과.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim*2)\n",
        "        logits = self.fc(lstm_out)  # (batch_size, seq_length, output_dim)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ucW2asV1m4ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(padded_X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(padded_y_train, dtype=torch.long)\n",
        "X_valid_tensor = torch.tensor(padded_X_valid, dtype=torch.long)\n",
        "y_valid_tensor = torch.tensor(padded_y_valid, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(padded_X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(padded_y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "NU-JmUc7eqJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "etFuvh0ver4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2+len(words_counts)\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = len(bag)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "num_layers = 4\n",
        "\n",
        "model = NERTagger(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QI_3nfReuzv",
        "outputId": "9bfcf45d-7cf2-4d8d-bc5e-ca17a3d6b172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NERTagger(\n",
              "  (embedding): Embedding(21011, 100)\n",
              "  (lstm): LSTM(100, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=46, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "RquGYTZ9fGy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(logits, labels, ignore_index=0):\n",
        "    predicted = torch.argmax(logits, dim=1)\n",
        "    mask = (labels != ignore_index)\n",
        "    correct = (predicted == labels).masked_select(mask).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "wGuEXfoXfI7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, valid_dataloader, criterion, device):\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in valid_dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            logits = model(batch_X)\n",
        "\n",
        "            loss = criterion(logits.view(-1, output_dim), batch_y.view(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_correct += calculate_accuracy(logits.view(-1, output_dim), batch_y.view(-1)) * batch_y.size(0)\n",
        "            val_total += batch_y.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss /= len(valid_dataloader)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "28_INq47fKGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "bUQyws3iN1-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_X, batch_y in tqdm(train_dataloader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        logits = model(batch_X)\n",
        "\n",
        "        loss = criterion(logits.view(-1, output_dim), batch_y.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += calculate_accuracy(logits.view(-1, output_dim), batch_y.view(-1)) * batch_y.size(0)\n",
        "        train_total += batch_y.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "M8BYjGrAfLv6",
        "outputId": "84490c61-b134-4ed2-ce25-bf88e08d42bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:25<00:00, 16.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Train Loss: 1.2455, Train Accuracy: 0.6340\n",
            "Validation Loss: 0.6731, Validation Accuracy: 0.8007\n",
            "Validation loss improved from inf to 0.6731. 체크포인트를 저장합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10:\n",
            "Train Loss: 0.4726, Train Accuracy: 0.8589\n",
            "Validation Loss: 0.5198, Validation Accuracy: 0.8513\n",
            "Validation loss improved from 0.6731 to 0.5198. 체크포인트를 저장합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10:\n",
            "Train Loss: 0.2882, Train Accuracy: 0.9156\n",
            "Validation Loss: 0.4292, Validation Accuracy: 0.8791\n",
            "Validation loss improved from 0.5198 to 0.4292. 체크포인트를 저장합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10:\n",
            "Train Loss: 0.1856, Train Accuracy: 0.9456\n",
            "Validation Loss: 0.5231, Validation Accuracy: 0.8732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10:\n",
            "Train Loss: 0.1204, Train Accuracy: 0.9645\n",
            "Validation Loss: 0.5799, Validation Accuracy: 0.8752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 73/439 [00:03<00:19, 18.76it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-02181bff0085>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "model.to(device)\n",
        "val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
        "\n",
        "print(f'Best model validation loss: {val_loss:.4f}')\n",
        "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNXLCpuefO6c",
        "outputId": "453e9e7c-99c0-4de2-8c1a-55adf863cd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model validation loss: 0.4292\n",
            "Best model validation accuracy: 0.8791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "print(f'Best model test loss: {test_loss:.4f}')\n",
        "print(f'Best model test accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOwr3jCHfgcQ",
        "outputId": "b3f358ff-9f13-43fd-bf1c-e3310f5c489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model test loss: 0.5189\n",
            "Best model test accuracy: 0.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu"
      ],
      "metadata": {
        "id": "-bBUrw15LbFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU 스코어 계산\n",
        "sum_bleu = 0\n",
        "for k in tqdm(range(len(test_text))):\n",
        "  input_tensor = torch.tensor(padded_X_test[k], dtype=torch.long).unsqueeze(0).to(device)\n",
        "  model.eval()\n",
        "  logits = model(input_tensor)\n",
        "  predicted_indices = torch.argmax(logits, dim=-1).squeeze(0).tolist()[:len(test_text[k])]\n",
        "  sum_bleu += bleu.sentence_bleu([predicted_indices],test_pos[k])\n",
        "print('\\nBLEU: ', end='')\n",
        "print(sum_bleu/len(test_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-au06WZi58_",
        "outputId": "8e1b9af3-2bfb-42b7-b7bb-cd20d5eeb9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3453 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "  0%|          | 11/3453 [00:00<00:32, 105.99it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 3453/3453 [00:38<00:00, 88.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5763728172294659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\"“Midcontinent prices were similarly lower in the $ 3.40s . New York city gate gas slipped into the % 4.40s , down almost 15 cents .”\".lower().split(' ')]\n",
        "encoded_sample = t2seq(sample_text, w2i)\n",
        "padded_sample = pad_seq(encoded_sample, max_len=max_len)\n",
        "input_tensor = torch.tensor(padded_sample[0], dtype=torch.long).unsqueeze(0).to(device)\n",
        "logits = model(input_tensor)\n",
        "predicted_indices = np.array(torch.argmax(logits, dim=-1).squeeze(0).tolist()[:len(encoded_sample[0])])"
      ],
      "metadata": {
        "id": "iRDXG5YM_Udq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_=[]\n",
        "for num in predicted_indices:\n",
        "  pos_.append(bag[num])\n",
        "print(list(zip(sample_text[0], pos_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDdHjO4PBvuP",
        "outputId": "7abe9c1d-d640-4d85-f5d5-71e37da77240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('“midcontinent', 'NN'), ('prices', 'NNS'), ('were', 'VBD'), ('similarly', 'RBR'), ('lower', 'JJR'), ('in', 'IN'), ('the', 'DT'), ('$', '$'), ('3.40s', 'CD'), ('.', 'JJS'), ('new', 'NNP'), ('york', 'NNP'), ('city', 'NNP'), ('gate', 'NNP'), ('gas', 'NNP'), ('slipped', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('%', 'JJ'), ('4.40s', 'NN'), (',', ','), ('down', 'RB'), ('almost', 'RB'), ('15', 'CD'), ('cents', 'NNS'), ('.”', 'CD')]\n"
          ]
        }
      ]
    }
  ]
}