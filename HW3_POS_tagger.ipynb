{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fxFORMoSO-C6bcMr7GsGcWQZc88_j1FP",
      "authorship_tag": "ABX9TyPgxB3Jsp+jaqR+PnRRIABv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT_87J1f_oQU",
        "outputId": "9ad2f207-d633-4cde-bbeb-160d44fff5b3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "QwlvzEOW76QW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def txt2list(filename):\n",
        "  f = open(filename, 'r')\n",
        "  lines = f.readlines()\n",
        "  f.close()\n",
        "  raw = ''.join(lines)\n",
        "  sents_raw = raw.split('\\n\\n')\n",
        "  words_raw=[]\n",
        "  for sent in sents_raw:\n",
        "    words_raw.append(sent.split('\\n'))\n",
        "  words_raw=words_raw[:-1]\n",
        "  tokenized_list = []\n",
        "  for words in words_raw:\n",
        "    w_list, pos_list = [],[]\n",
        "    for word in words:\n",
        "      w, pos = tuple(word.split('\\t'))\n",
        "      w_list.append(w.lower())\n",
        "      pos_list.append(pos)\n",
        "    tokenized_list.append((w_list, pos_list))\n",
        "  return tokenized_list"
      ],
      "metadata": {
        "id": "Vp5mPSOk5f3V"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/train.txt')\n",
        "valid_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/valid.txt')\n",
        "test_list = txt2list('/content/drive/MyDrive/Colab Notebooks/POS_tagger/test.txt')"
      ],
      "metadata": {
        "id": "60FSztT76HO5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxk2VvFV6KWY",
        "outputId": "e6125b1e-7cc6-4d36-f65a-49d1741bfb0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['swansea', '1', 'lincoln', '2'], ['NN', 'CD', 'NNP', 'CD'])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list2textpos(list_):\n",
        "  text, pos = [], []\n",
        "  for t in list_:\n",
        "    text.append(t[0])\n",
        "    pos.append(t[1])\n",
        "  return text, pos"
      ],
      "metadata": {
        "id": "z5vN4hA_JWwg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_text, train_pos), (test_text, test_pos), (valid_text, valid_pos) = tuple(map(list2textpos, (train_list, test_list, valid_list)))"
      ],
      "metadata": {
        "id": "BJUqMdyYKSkb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_bag=[]\n",
        "for p in train_pos: pos_bag+=p\n",
        "bag = ['<PAD>']+list(set(pos_bag))"
      ],
      "metadata": {
        "id": "UJg0r_TjJKA7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS\n"
      ],
      "metadata": {
        "id": "NTD6NnrG-1xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BOheDaW8K1",
        "outputId": "6c5b148f-a453-4fcd-cf03-2298d1b77e24"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<PAD>',\n",
              " 'VBZ',\n",
              " 'WP$',\n",
              " ':',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'POS',\n",
              " 'SYM',\n",
              " 'RBR',\n",
              " 'EX',\n",
              " 'CD',\n",
              " 'LS',\n",
              " 'NN|SYM',\n",
              " 'DT',\n",
              " 'VBN',\n",
              " 'PDT',\n",
              " 'WP',\n",
              " 'JJR',\n",
              " 'RP',\n",
              " 'RBS',\n",
              " 'WDT',\n",
              " 'NNP',\n",
              " 'UH',\n",
              " 'MD',\n",
              " 'JJS',\n",
              " 'WRB',\n",
              " ',',\n",
              " 'CC',\n",
              " '$',\n",
              " 'NNPS',\n",
              " 'TO',\n",
              " \"''\",\n",
              " 'IN',\n",
              " 'VBP',\n",
              " 'VBG',\n",
              " ')',\n",
              " 'VB',\n",
              " 'PRP$',\n",
              " 'FW',\n",
              " 'NN',\n",
              " 'JJ',\n",
              " '\"',\n",
              " '(',\n",
              " 'RB',\n",
              " 'VBD']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos2label(pos):\n",
        "  pos_=[]\n",
        "  for pos_list in pos:\n",
        "    pos_.append(list(map(bag.index, pos_list)))\n",
        "  return pos_"
      ],
      "metadata": {
        "id": "wMjfp2fZLLqk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_pos, test_pos, valid_pos) = tuple(map(pos2label, (train_pos, test_pos, valid_pos)))"
      ],
      "metadata": {
        "id": "WYlx5GhcLwFQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VOCAB"
      ],
      "metadata": {
        "id": "6LhtoDq6dWLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "words_list=[]\n",
        "for t in train_text:\n",
        "  words_list+=t\n",
        "words_counts = Counter(words_list)"
      ],
      "metadata": {
        "id": "YFTPUfeFMh-f"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(words_counts, key=words_counts.get, reverse=True)"
      ],
      "metadata": {
        "id": "0Po3lSWhRo-V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2i={}\n",
        "for index, word in enumerate(['<PAD>', '<UNK>']+vocab):\n",
        "  w2i[word] = index"
      ],
      "metadata": {
        "id": "igVP9cBnR2Ra"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def t2seq(texts, w2i):\n",
        "  encoded_texts=[]\n",
        "  for text in texts:\n",
        "    index_seq = []\n",
        "    for word in text:\n",
        "      try: index_seq.append(w2i[word])\n",
        "      except KeyError: index_seq.append(w2i['<UNK>'])\n",
        "    encoded_texts.append(index_seq)\n",
        "  return encoded_texts"
      ],
      "metadata": {
        "id": "jR-Knnf2TC6I"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_X_train = t2seq(train_text, w2i)\n",
        "encoded_y_train = train_pos\n",
        "encoded_X_test = t2seq(test_text, w2i)\n",
        "encoded_y_test = test_pos\n",
        "encoded_X_valid = t2seq(valid_text, w2i)\n",
        "encoded_y_valid = valid_pos"
      ],
      "metadata": {
        "id": "-HKJVjraUv5L"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(map(len, encoded_X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfNQU-8bOat",
        "outputId": "b31f370d-c82f-4f31-cd12-5171825f13bc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=120\n",
        "\n",
        "def pad_seq(encoded, max_len):\n",
        "  features = np.zeros((len(encoded), max_len), dtype=int)\n",
        "  for i, sent in enumerate(encoded):\n",
        "    features[i, :len(sent)] = np.array(sent)[:max_len]\n",
        "  return features"
      ],
      "metadata": {
        "id": "Tpx7FawOaTQP"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_X_train = pad_seq(encoded_X_train, max_len=max_len)\n",
        "padded_y_train = pad_seq(encoded_y_train, max_len=max_len)\n",
        "padded_X_test = pad_seq(encoded_X_test, max_len=max_len)\n",
        "padded_y_test = pad_seq(encoded_y_test, max_len=max_len)\n",
        "padded_X_valid = pad_seq(encoded_X_valid, max_len=max_len)\n",
        "padded_y_valid = pad_seq(encoded_y_valid, max_len=max_len)"
      ],
      "metadata": {
        "id": "YELsmnMUbtwH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmNGaTfxtNzL",
        "outputId": "ada0c65c-605b-4d94-dfbe-0abaa4fffbf1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[989, 10951, 205, 629, 7, 3939, 216, 5774, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkVzRVZ6s-lg",
        "outputId": "569e7c34-7fcc-4e4e-b53b-f9aaa9089bdf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22, 1, 41, 40, 31, 37, 41, 40, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bidirectional LSTM**"
      ],
      "metadata": {
        "id": "pEraFXYj--Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=2):\n",
        "        super(NERTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ucW2asV1m4ZO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(padded_X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(padded_y_train, dtype=torch.long)\n",
        "X_valid_tensor = torch.tensor(padded_X_valid, dtype=torch.long)\n",
        "y_valid_tensor = torch.tensor(padded_y_valid, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(padded_X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(padded_y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "NU-JmUc7eqJ0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "etFuvh0ver4u"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2+len(words_counts)\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = len(bag)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "num_layers = 4\n",
        "\n",
        "model = NERTagger(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QI_3nfReuzv",
        "outputId": "ca7a698d-42d9-46c3-8420-a67e86722133"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NERTagger(\n",
              "  (embedding): Embedding(21011, 100)\n",
              "  (lstm): LSTM(100, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=46, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "RquGYTZ9fGy1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(logits, labels, ignore_index=0):\n",
        "    predicted = torch.argmax(logits, dim=1)\n",
        "    mask = (labels != ignore_index)\n",
        "    correct = (predicted == labels).masked_select(mask).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "wGuEXfoXfI7-"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, valid_dataloader, criterion, device):\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in valid_dataloader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            logits = model(batch_X)\n",
        "\n",
        "            loss = criterion(logits.view(-1, output_dim), batch_y.view(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_correct += calculate_accuracy(logits.view(-1, output_dim), batch_y.view(-1)) * batch_y.size(0)\n",
        "            val_total += batch_y.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss /= len(valid_dataloader)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "28_INq47fKGI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "bUQyws3iN1-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch_X, batch_y in tqdm(train_dataloader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        logits = model(batch_X)\n",
        "\n",
        "        loss = criterion(logits.view(-1, output_dim), batch_y.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += calculate_accuracy(logits.view(-1, output_dim), batch_y.view(-1)) * batch_y.size(0)\n",
        "        train_total += batch_y.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}')\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8BYjGrAfLv6",
        "outputId": "7378c604-0f26-42fd-81ba-71f43534d626"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:33<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Train Loss: 1.2889, Train Accuracy: 0.6250\n",
            "Validation Loss: 0.6097, Validation Accuracy: 0.8211\n",
            "Validation loss improved from inf to 0.6097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10:\n",
            "Train Loss: 0.4776, Train Accuracy: 0.8586\n",
            "Validation Loss: 0.4308, Validation Accuracy: 0.8733\n",
            "Validation loss improved from 0.6097 to 0.4308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:22<00:00, 19.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10:\n",
            "Train Loss: 0.2927, Train Accuracy: 0.9140\n",
            "Validation Loss: 0.3821, Validation Accuracy: 0.8881\n",
            "Validation loss improved from 0.4308 to 0.3821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:23<00:00, 18.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10:\n",
            "Train Loss: 0.1873, Train Accuracy: 0.9452\n",
            "Validation Loss: 0.4017, Validation Accuracy: 0.8900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:23<00:00, 18.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10:\n",
            "Train Loss: 0.1196, Train Accuracy: 0.9656\n",
            "Validation Loss: 0.4129, Validation Accuracy: 0.8924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:23<00:00, 18.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10:\n",
            "Train Loss: 0.0802, Train Accuracy: 0.9765\n",
            "Validation Loss: 0.4304, Validation Accuracy: 0.8964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:23<00:00, 18.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10:\n",
            "Train Loss: 0.0515, Train Accuracy: 0.9848\n",
            "Validation Loss: 0.4465, Validation Accuracy: 0.8997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:24<00:00, 18.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10:\n",
            "Train Loss: 0.0349, Train Accuracy: 0.9896\n",
            "Validation Loss: 0.4775, Validation Accuracy: 0.8998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:27<00:00, 16.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10:\n",
            "Train Loss: 0.0300, Train Accuracy: 0.9910\n",
            "Validation Loss: 0.5194, Validation Accuracy: 0.8976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 439/439 [00:24<00:00, 18.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10:\n",
            "Train Loss: 0.0223, Train Accuracy: 0.9933\n",
            "Validation Loss: 0.5245, Validation Accuracy: 0.9003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VALIDATION"
      ],
      "metadata": {
        "id": "wPb-OFSeQeKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
        "model.to(device)\n",
        "val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
        "\n",
        "print(f'Best model validation loss: {val_loss:.4f}')\n",
        "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNXLCpuefO6c",
        "outputId": "c601c022-995d-4b1f-fbad-79784a3b71ff"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model validation loss: 0.3821\n",
            "Best model validation accuracy: 0.8881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "print(f'Best model test loss: {test_loss:.4f}')\n",
        "print(f'Best model test accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOwr3jCHfgcQ",
        "outputId": "944dc343-4967-4a4e-b05b-70bed62bce57"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model test loss: 0.4375\n",
            "Best model test accuracy: 0.8746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu"
      ],
      "metadata": {
        "id": "-bBUrw15LbFt"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "zvrIBgszQcCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_bleu = 0\n",
        "for k in tqdm(range(len(test_text))):\n",
        "  input_tensor = torch.tensor(padded_X_test[k], dtype=torch.long).unsqueeze(0).to(device)\n",
        "  model.eval()\n",
        "  logits = model(input_tensor)\n",
        "  predicted_indices = torch.argmax(logits, dim=-1).squeeze(0).tolist()[:len(test_text[k])]\n",
        "  sum_bleu += bleu.sentence_bleu([predicted_indices],test_pos[k])\n",
        "print('\\nBLEU: ', end='')\n",
        "print(sum_bleu/len(test_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-au06WZi58_",
        "outputId": "e919c1f8-f6de-4b9e-d27f-8fb81f01bd9c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3453 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 3453/3453 [00:45<00:00, 76.07it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU: 0.6151803071137855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\"“Midcontinent prices were similarly lower in the $ 3.40s . New York city gate gas slipped into the % 4.40s , down almost 15 cents .”\".lower().split(' ')]\n",
        "encoded_sample = t2seq(sample_text, w2i)\n",
        "padded_sample = pad_seq(encoded_sample, max_len=max_len)\n",
        "input_tensor = torch.tensor(padded_sample[0], dtype=torch.long).unsqueeze(0).to(device)\n",
        "logits = model(input_tensor)\n",
        "predicted_indices = np.array(torch.argmax(logits, dim=-1).squeeze(0).tolist()[:len(encoded_sample[0])])"
      ],
      "metadata": {
        "id": "iRDXG5YM_Udq"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_=[]\n",
        "for num in predicted_indices:\n",
        "  pos_.append(bag[num])\n",
        "print(list(zip(sample_text[0], pos_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDdHjO4PBvuP",
        "outputId": "871bd142-bbd0-4a96-91cd-f71baa2e0081"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('“midcontinent', 'NNP'), ('prices', 'NNS'), ('were', 'VBD'), ('similarly', 'JJ'), ('lower', 'JJR'), ('in', 'IN'), ('the', 'DT'), ('$', '$'), ('3.40s', 'CD'), ('.', 'NN'), ('new', 'NNP'), ('york', 'NNP'), ('city', 'NNP'), ('gate', 'NNP'), ('gas', 'NNP'), ('slipped', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('%', 'NN'), ('4.40s', 'NN'), (',', ','), ('down', 'RB'), ('almost', 'RB'), ('15', 'CD'), ('cents', 'NNS'), ('.”', 'CD')]\n"
          ]
        }
      ]
    }
  ]
}